{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YJbCUbf7q2Fz",
        "outputId": "457d5fa5-89a7-408d-bab1-6075a452a90b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Charan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "C:\\Users\\Charan\\AppData\\Local\\Temp\\ipykernel_18508\\1722568904.py:22: DtypeWarning: Columns (6,7,8,9,10,11,12,13,15,16,17,18,19,20,21,22,23,25,26,28,29,30,32,33,34,35,36,37,38,39,40,41,42,43,44,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,270,271,272,273,274,275,276,277,278,279,280,281) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv(filepath, encoding='latin1')  # or encoding='ISO-8859-1'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column names: Index(['author', 'date', 'headlines', 'read_more', 'text', 'ctext',\n",
            "       'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9',\n",
            "       ...\n",
            "       'Unnamed: 272', 'Unnamed: 273', 'Unnamed: 274', 'Unnamed: 275',\n",
            "       'Unnamed: 276', 'Unnamed: 277', 'Unnamed: 278', 'Unnamed: 279',\n",
            "       'Unnamed: 280', 'Unnamed: 281'],\n",
            "      dtype='object', length=282)\n"
          ]
        },
        {
          "ename": "FeatureNotFound",
          "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 159\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_column \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m summary_column \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 159\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst few rows after preprocessing:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
            "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(data, text_column, summary_column)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(data, text_column, summary_column):\n\u001b[1;32m---> 36\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[summary_column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: clean_text(x))\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[1;32md:\\PROJECTS\\News Summarizer\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\PROJECTS\\News Summarizer\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\PROJECTS\\News Summarizer\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
            "File \u001b[1;32md:\\PROJECTS\\News Summarizer\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\PROJECTS\\News Summarizer\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
            "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
            "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mpreprocess_data.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(data, text_column, summary_column):\n\u001b[1;32m---> 36\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[text_column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[summary_column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: clean_text(x))\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n\u001b[1;32m---> 27\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext  \u001b[38;5;66;03m# Remove HTML tags\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m([^)]*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove text in parentheses\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove quotations\u001b[39;00m\n",
            "File \u001b[1;32md:\\PROJECTS\\News Summarizer\\.venv\\Lib\\site-packages\\bs4\\__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "\n",
        "# Ensure NLTK data is downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 1. Data Collection\n",
        "def load_data(filepath):\n",
        "    data = pd.read_csv(filepath, encoding='latin1')  # or encoding='ISO-8859-1'\n",
        "    return data\n",
        "\n",
        "# 2. Data Processing\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text  # Remove HTML tags\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)  # Remove text in parentheses\n",
        "    text = re.sub('\"', '', text)  # Remove quotations\n",
        "    text = re.sub(r\"'s\\b\", \"\", text)\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)  # Remove special characters and digits\n",
        "    text = text.lower()  # Lowercase all characters\n",
        "    return text\n",
        "\n",
        "def preprocess_data(data, text_column, summary_column):\n",
        "    data['cleaned_text'] = data[text_column].apply(lambda x: clean_text(x))\n",
        "    data['cleaned_summary'] = data[summary_column].apply(lambda x: clean_text(x))\n",
        "    return data\n",
        "\n",
        "# 3. Exploratory Data Analysis (EDA)\n",
        "def eda(data):\n",
        "    data_len = [len(s.split()) for s in data['cleaned_text']]\n",
        "    plt.hist(data_len, bins=30)\n",
        "    plt.xlabel('Number of Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# 4. Text Preprocessing\n",
        "def tokenize_text(data, max_len):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(data)\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "    padded = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "    return tokenizer, padded\n",
        "\n",
        "def preprocess_text(data):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    data['cleaned_text'] = data['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "    data['cleaned_summary'] = data['cleaned_summary'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "    return data\n",
        "\n",
        "# 5. Abstractive Summarization Model (Seq2Seq)\n",
        "def build_model(vocab_size, max_len_text, max_len_summary):\n",
        "    embedding_dim = 300\n",
        "    latent_dim = 500\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_len_text,))\n",
        "    enc_emb = Embedding(vocab_size, embedding_dim, trainable=True)(encoder_inputs)\n",
        "\n",
        "    # LSTM 1\n",
        "    encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "    # LSTM 2\n",
        "    encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "    # LSTM 3\n",
        "    encoder_lstm3 = LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    dec_emb_layer = Embedding(vocab_size, embedding_dim, trainable=True)\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "    # LSTM using encoder_states as initial state\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Dense layer\n",
        "    decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# 6. Model Training\n",
        "def train_model(model, x_train, y_train, x_val, y_val, batch_size, epochs):\n",
        "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "    model.fit([x_train['text'], x_train['summary'][:, :-1]],\n",
        "              y_train['summary'][:, 1:],\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=([x_val['text'], x_val['summary'][:, :-1]], y_val['summary'][:, 1:]))\n",
        "    return model\n",
        "\n",
        "# 7. Evaluation using ROUGE, BLEU\n",
        "def evaluate_model(model, x_val, y_val, tokenizer, max_len_summary):\n",
        "    y_pred = model.predict([x_val['text'], x_val['summary'][:, :-1]])\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    rouge = Rouge()\n",
        "    references = [' '.join([tokenizer.index_word[i] for i in y if i != 0]) for y in y_val['summary'][:, 1:]]\n",
        "    hypotheses = [' '.join([tokenizer.index_word[i] for i in y if i != 0]) for y in y_pred]\n",
        "\n",
        "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
        "\n",
        "    bleu_scores = []\n",
        "    for i in range(len(y_pred)):\n",
        "        ref = [tokenizer.index_word[j] for j in y_val['summary'][i, 1:] if j != 0]\n",
        "        pred = [tokenizer.index_word[j] for j in y_pred[i] if j != 0]\n",
        "        bleu_scores.append(sentence_bleu([ref], pred))\n",
        "\n",
        "    return scores, np.mean(bleu_scores)\n",
        "\n",
        "# 8. Model Testing\n",
        "def test_model(model, text, tokenizer, max_len_text, max_len_summary):\n",
        "    text = clean_text(text)\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_len_text, padding='post')\n",
        "\n",
        "    pred = model.predict([padded, np.zeros((1, max_len_summary))])\n",
        "    pred = np.argmax(pred, axis=-1)\n",
        "\n",
        "    summary = ' '.join([tokenizer.index_word[i] for i in pred[0] if i != 0])\n",
        "    return summary\n",
        "\n",
        "# Load the data\n",
        "filepath = 'd:\\\\PROJECTS\\\\News Summarizer\\\\news_summary.csv'  # Change this to the correct path\n",
        "data = load_data(filepath)\n",
        "\n",
        "# Print column names to verify correct column names\n",
        "print(\"Column names:\", data.columns)\n",
        "\n",
        "# Columns containing the text and summary\n",
        "text_column = 'text'\n",
        "summary_column = 'headlines'\n",
        "\n",
        "# Check if the specified columns exist in the DataFrame\n",
        "if text_column not in data.columns or summary_column not in data.columns:\n",
        "    raise KeyError(f\"Columns '{text_column}' or '{summary_column}' not found in the dataset.\")\n",
        "\n",
        "data = preprocess_data(data, text_column=text_column, summary_column=summary_column)\n",
        "print(\"First few rows after preprocessing:\")\n",
        "print(data.head())\n",
        "\n",
        "eda(data)\n",
        "\n",
        "max_len_text = 300\n",
        "max_len_summary = 50\n",
        "\n",
        "data = preprocess_text(data)\n",
        "tokenizer, padded_texts = tokenize_text(data['cleaned_text'], max_len_text)\n",
        "_, padded_summaries = tokenize_text(data['cleaned_summary'], max_len_summary)\n",
        "\n",
        "print(\"Shape of padded_texts:\", padded_texts.shape)\n",
        "print(\"Shape of padded_summaries:\", padded_summaries.shape)\n",
        "\n",
        "# Ensure padded_summaries is in the correct format\n",
        "padded_summaries = np.array(padded_summaries)\n",
        "\n",
        "# Perform train-test split\n",
        "text_train, text_val, summary_train, summary_val = train_test_split(\n",
        "    padded_texts, padded_summaries, test_size=0.2, random_state=0)\n",
        "\n",
        "# Create dictionaries for training and validation sets\n",
        "x_train = {'text': text_train, 'summary': summary_train}\n",
        "x_val = {'text': text_val, 'summary': summary_val}\n",
        "y_train = {'text': text_train, 'summary': summary_train}\n",
        "y_val = {'text': text_val, 'summary': summary_val}\n",
        "\n",
        "print(\"Shapes of train and validation sets:\")\n",
        "print(\"x_train:\", {k: v.shape for k, v in x_train.items()})\n",
        "print(\"y_train:\", {k: v.shape for k, v in y_train.items()})\n",
        "print(\"x_val:\", {k: v.shape for k, v in x_val.items()})\n",
        "print(\"y_val:\", {k: v.shape for k, v in y_val.items()})\n",
        "\n",
        "model = build_model(len(tokenizer.word_index) + 1, max_len_text, max_len_summary)\n",
        "model = train_model(model, x_train, y_train, x_val, y_val, batch_size=64, epochs=10)\n",
        "\n",
        "scores, bleu_score = evaluate_model(model, x_val, y_val, tokenizer, max_len_summary)\n",
        "print('ROUGE Scores:', scores)\n",
        "print('BLEU Score:', bleu_score)\n",
        "\n",
        "# Test the model\n",
        "test_text = \"Your input text here.\"\n",
        "print('Summary:', test_model(model, test_text, tokenizer, max_len_text, max_len_summary))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIwljNsPrB6R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
